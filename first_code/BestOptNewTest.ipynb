{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Note: Environment variables set for this entire notebook session\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_DISABLE_METAL\"] = \"1\"  # Mac-specific\n",
    "\n",
    "import tensorflow as tf\n",
    "# Explicitly disable GPU devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Verify\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Note: Environment variables set for this entire notebook session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test matrix multiplication result:\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n",
      "Device used: /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# Quick test to confirm CPU-only execution\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    \n",
    "print(\"Test matrix multiplication result:\")\n",
    "print(c)\n",
    "print(\"Device used:\", c.device)  # Should show CPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pkl files\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open('./Input/8-mers/mic_dframe.pkl', 'rb') as file:\n",
    "    mic_dframe = pickle.load(file)\n",
    "with open('./Input/8-mers/suscep_classes.pkl', 'rb') as file:\n",
    "    suscep_classes = pd.read_pickle(file)\n",
    "    \n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "antibiotics = mic_dframe.columns[-12:]\n",
    "\n",
    "def prob_to_onehot(y):\n",
    "    for i in range(0, y.shape[0]):\n",
    "        greater = 0\n",
    "        pos = 0\n",
    "        for j in range(0, y.shape[1]):\n",
    "            if y[i][j] >= greater:\n",
    "                greater = y[i][j]\n",
    "                pos = j\n",
    "            y[i][j] = 0.0\n",
    "        y[i][pos] = 1.0\n",
    "    return y\n",
    "\n",
    "def best_N_features(target_df, antibiotic, N):\n",
    "    path = './input/8-mers/counts/'\n",
    "    genome_ids = target_df['Genome ID'].loc[pd.notnull(target_df[antibiotic])].values\n",
    "    X = np.array([np.load(path + genome_id + '.npy') for genome_id in genome_ids])\n",
    "    y = target_df[antibiotic].loc[pd.notnull(target_df[antibiotic])].values\n",
    "    n_best = SelectKBest(chi2, k=N)\n",
    "    X_new = n_best.fit_transform(X, y*10**5)\n",
    "    kmers = np.load('./input/8-mers/kmers_basis.npy')\n",
    "    selected_kmers = [column[0]  for column in zip(kmers, n_best.get_support()) if column[1]]\n",
    "    #scores = k_best.fit(X,y).scores_\n",
    "    best_feature_df = pd.DataFrame(X_new, columns = selected_kmers)\n",
    "    return best_feature_df\n",
    "\n",
    "def class_weighting(df, antibiotic, cv):\n",
    "    # Unique mic values\n",
    "    mics = df[antibiotic].loc[pd.notnull(df[antibiotic])].unique()\n",
    "    # Samples per class\n",
    "    samples = {mic : len(df.loc[df[antibiotic]==mic]) for mic in mics}\n",
    "    # Sorted classes\n",
    "    mics = sorted([key for key in samples.keys()])\n",
    "    # total data\n",
    "    total = len(df.loc[pd.notnull(df[antibiotic])])\n",
    "    # class weights\n",
    "    class_weight = {i: (1 / samples[mic])*(total/len(mics))*(1/cv) for i, mic in enumerate(mics)}\n",
    "    \n",
    "    return class_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ciprofloxacin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'ciprofloxacin'\n",
    "\n",
    "if \n",
    "    \n",
    "\n",
    "    kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "    # Features\n",
    "    X = kmer_dframe.values\n",
    "\n",
    "    # Standardize the input data\n",
    "    scaler_X = StandardScaler().fit(X)\n",
    "    X_scaled = scaler_X.transform(X)\n",
    "\n",
    "    # Target\n",
    "    # list of MIC values\n",
    "    y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "    # reshape the list of mics\n",
    "    y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "    # define encoder function\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # transform the target categorical data to onehot code\n",
    "    y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "    # Split into the training and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "    # Model building functionu\n",
    "    def make_model_ciprofloxacin(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "        \n",
    "        model = tf.keras.Sequential()\n",
    "        initializer = tf.keras.initializers.HeNormal()\n",
    "        learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                        use_bias=False, input_shape=[X.shape[1]]))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                    center=True, scale=True, \n",
    "                                                    beta_initializer='zeros', \n",
    "                                                    gamma_initializer='ones', \n",
    "                                                    moving_mean_initializer='zeros', \n",
    "                                                    moving_variance_initializer='ones'))\n",
    "        model.add(tf.keras.layers.Dropout(0.7))\n",
    "        model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "        \n",
    "\n",
    "        # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Include an early stopping callback for convenience\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        # monitor (loss or val_loss)\n",
    "        monitor='val_loss',\n",
    "        # how many epochs to wait before stopping (minimum epochs)\n",
    "        patience=150,\n",
    "        # minimium amount of change to count as an improvement\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    starttime = time.time()\n",
    "\n",
    "    class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "    # Define the model\n",
    "    model = make_model_ciprofloxacin()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        shuffle=True,\n",
    "        validation_split=0.3,\n",
    "        #validation_data=(X_valid, y_valid),\n",
    "        batch_size=512,\n",
    "        epochs=5000,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight = class_weight,\n",
    "        verbose=0, # hide the output because we have so many epochs\n",
    "    )\n",
    "\n",
    "    print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = prob_to_onehot(y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "    # Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    # Start the plot at epoch 5\n",
    "    history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "    history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "    print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "        \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "        .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "                history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "    # Make predictions and decode the OneHot\n",
    "    mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "    # True mic values in string format\n",
    "    mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "    # Unique mic values\n",
    "    mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "    # Plot the cm\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trimethoprim/sulfamethoxazole**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'trimethoprim/sulfamethoxazole'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_trimethoprim_sulfamethoxazole(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.007),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=150,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_trimethoprim_sulfamethoxazole()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=2000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ceftriaxone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'ceftriaxone'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_ceftriaxone(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=100,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_ceftriaxone()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=2000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gentamicin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'gentamicin'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_gentamicin(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=1000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.7))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=500,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_gentamicin()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=4000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ceftiofur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'ceftiofur'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_ceftiofur(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=150,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_ceftiofur()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ampicillin**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'ampicillin'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_ampicillin(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=100,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_ampicillin()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**amoxicillin/clavulanic acid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'amoxicillin/clavulanic acid'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_amoxicillin(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.7))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=600,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_amoxicillin()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cefoxitin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'cefoxitin'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_cefoxitin(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=100000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=1000,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_cefoxitin()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=2500,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nalidixic acid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'nalidixic acid'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "# Model building function\n",
    "def make_model_nalidixic_acid(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = 0.00001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=500,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_nalidixic_acid()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=7000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tetracycline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antibiotic = 'tetracycline'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Model building function\n",
    "def make_model_tetracycline(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.7))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=100,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_tetracycline()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1200,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chloramphenicol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'chloramphenicol'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Model building function\n",
    "def make_model_chloramphenicol(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.7))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=100,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_chloramphenicol()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sulfisoxazole**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibiotic = 'sulfisoxazole'\n",
    "\n",
    "kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=2000)\n",
    "\n",
    "# Features\n",
    "X = kmer_dframe.values\n",
    "\n",
    "# Standardize the input data\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "\n",
    "# Target\n",
    "# list of MIC values\n",
    "y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n",
    "\n",
    "# reshape the list of mics\n",
    "y_reshape = np.reshape(y,(-1,1))\n",
    "\n",
    "# define encoder function\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# transform the target categorical data to onehot code\n",
    "y_onehot = encoder.fit_transform(y_reshape)\n",
    "\n",
    "# Split into the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Model building function\n",
    "def make_model_sulfisoxazole(loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                    use_bias=False, input_shape=[X.shape[1]]))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001,\n",
    "                                                center=True, scale=True, \n",
    "                                                beta_initializer='zeros', \n",
    "                                                gamma_initializer='ones', \n",
    "                                                moving_mean_initializer='zeros', \n",
    "                                                moving_variance_initializer='ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.7))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Add the cross-entropy loss and accuracy metric for threshold probability\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Include an early stopping callback for convenience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor (loss or val_loss)\n",
    "    monitor='val_loss',\n",
    "    # how many epochs to wait before stopping (minimum epochs)\n",
    "    patience=200,\n",
    "    # minimium amount of change to count as an improvement\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "class_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n",
    "# Define the model\n",
    "model = make_model_sulfisoxazole()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    #validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight = class_weight,\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")\n",
    "\n",
    "print('Training Time: {:0.2f} seconds'.format(time.time() - starttime))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = prob_to_onehot(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy: \" + str(accuracy))\n",
    "\n",
    "\n",
    "# Plot the loss (accuracy) and validation loss (accuracy) functions-----------------------------------------------------\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.2f}\" + \"\\nBest Validation Accuracy: {:0.2f}\" + \"\\nBest Train Loss: {:0.2f}\" + \n",
    "       \"\\nBest Train Accuracy: {:0.2f}\")\\\n",
    "      .format(history_df['val_loss'].min(), history_df['val_categorical_accuracy'].max(), history_df[\"loss\"].min(), \n",
    "              history_df[\"categorical_accuracy\"].max()))\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and decode the OneHot\n",
    "mics_pred = [str(item[0]) for item in encoder.inverse_transform(y_pred)]\n",
    "# True mic values in string format\n",
    "mics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n",
    "# Unique mic values\n",
    "mics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n",
    "# Plot the cm\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste com novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
